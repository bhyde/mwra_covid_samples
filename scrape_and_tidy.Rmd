---
title: "MWRA's data on RNA fragments of SARS-CoV-2 in the waste water."
output: html_document
---

# Disclaimer
Warning - amateur at work.  I know little to nothing about: virology, epidemology, RStudio, sewage treatment and so many other things.

# See Also
* "SARS-CoV-2 titers in wastewater foreshadow dynamics and clinical presentation of new COVID-19 cases" [doi:10.1101/2020.06.15.20117747](https://doi.org/10.1101/2020.06.15.20117747) A preprint.
* Github repo: [bhyde/mwra_covid_samples](https://github.com/bhyde/mwra_covid_samples).

# Introduction
Scroll to the bottom to see the charts.  Most of this is the R code to load, scrape, clean, normalize the data.

The Massachusetts Water Resources Agency or MWRA has a large sewage treatment facility (Deer Island) that handles the sewage generted by much of the area around Boston.  The inflow to this comes from two regions, north and south.  The Charles river divides these two.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install packages we use below, if necessary
for(pkg in c("tidyverse", "urltools", "rvest", "here",
             "feasts", "pdftools", "gridExtra")){
  if (!(pkg %in% installed.packages())) { install.packages(pkg)}}
library(tidyverse)
library(feasts)
library(urltools)
library(rvest)
library(here)
library(gt)
library(lubridate)
library(ggplot2)
library(scales)
library(gridExtra)
library(pdftools)
```
# Fetch, extract, tidy, create time series
## Fetch data: a pdf
We get the_data from the ["http://www.mwra.com/biobot/"](biobot page at MWRA's web site).  The actual data is in a linked pdf, so we have to scrape the URL for that before fetching it.
```{r fetch_data}
base_url = "http://www.mwra.com/biobot/"
latest_pdf_url = (paste0(base_url, "biobotdata.htm") 
                   %>% read_html()
                   %>% html_node("p > u > a")
                   %>% html_attr('href')
                   %>% paste0(base_url, .) )
if ( ! file.exists(here("data", basename(latest_pdf_url))) ){
  download.file(url=latest_pdf_url, 
                destfile=here("data", basename(latest_pdf_url)))
  file.remove(here("data", "mwra_samples.pdf"))
  file.symlink(
     basename(latest_pdf_url),
     here("data", "mwra_samples.pdf")) }
```
## Extract data: set of lines from a table
We use pdftools::pdf_text to get the content of the PDF.  As usual it is a PIA to tidy that up.
```{r extract_data}
pdf <- pdf_text(here("data", "mwra_samples.pdf"))

lines <- (read_lines(reduce(pdf[1:length(pdf)], paste0))    # pdf is N pages, merge them, break into lines
          %>% as_tibble                                     # enter the tidyverse
          %>% filter(grepl('^ *\\d+/\\d+/\\d+ ',value))     # retain only the lines reporting samples on a date
          %>% mutate(value=str_trim(value, side = "both"))  # we will split on space, but discard edge spaces
          %>% mutate(value=str_replace(value, "               ", "            NA"))  # too clever way to fill
          %>% mutate(value=str_replace(value, "(\\d)    (\\d)", "\\1 NA \\2"))       # in the empty celsl with
          %>% mutate(value=str_replace(value, "^    (\\d)", "NA \\1"))               # NA.
          %>% mutate(value=str_squish(value))
          %>% separate(value, sep=" ", into=c("date","northern", "southern", "a", "b", "c", "d"), fill="right")
          %>% select(date, northern, southern))             # discard the Excel computed averages
gt(tail(lines, 10)) %>% tab_header(title="Last few dates with samples.") %>% tab_source_note("Some days some samples aren't available.")
```
## Convert to Timeseries of samples
```{r infer_samples}
samples = (  # Convert lines into a table of samples
  bind_rows(
   tibble(date=lines$date, region="northern", rna=lines$northern),
   tibble(date=lines$date, region="southern", rna=lines$southern))
  %>% mutate(rna=parse_integer(rna), date=mdy(date))
  %>% drop_na() # dates with NA didn't have a sample for that region
  %>% as_tsibble(key=region, index=date)
  %>% arrange(date))
gt(tail(samples, 10)) %>% tab_header(title="Last ten samples")
```
# Display: some charts
Chart the data, twice once linear and one log.  Overlaid with a loess curve.  Why?  Because it's easy.
```{r plot_samples, fig.width = 6.5, fig.height = 7, include=TRUE}
theme_set(theme_minimal())
p_linear <- (samples %>% 
  group_by(region) %>% 
  ggplot(aes(x=date, y=rna, color=region, fill=region)) + 
  geom_point(size=1) + 
  geom_smooth(method="loess", formula= y ~ x, span = .2, alpha=.1) +
  theme(legend.position="top") +
  scale_x_date(date_breaks = "2 week", date_labels="%b %d") +
  labs(subtitle="linear scale"))
p_log <- (p_linear +  scale_y_log10() + 
  theme(legend.position="none") + labs(subtitle="log scale") + geom_rug(sides="t"))
grid.arrange(top="MWRA waste water: covid rna counts", p_linear, p_log)
```